{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64efb2d7-cf21-45c2-a6f1-92de51329651",
   "metadata": {},
   "source": [
    "# ðŸ“Š ETL Append Pipeline\n",
    "This notebook demonstrates an ETL pipeline that reads data from multiple file formats (CSV, JSON, XML), appends them vertically, and exports the result into a single CSV file.\n",
    "\n",
    "**ETL Stages Covered:**\n",
    "- **Extract**: Read from multiple formats\n",
    "- **Transform**: Combine data with consistent structure\n",
    "- **Load**: Save as a clean CSV file\n",
    "\n",
    "âœ… Built with reusability and logging in mind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d035a1-5fe8-4a71-88fc-672b6e4da3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a53c1d6-cc2c-49a4-848a-7a9f7454ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change paths as needed\n",
    "source_path = r\"F:\\projects\\coursera\\ibm data engineer_ETL_project\\source\"\n",
    "target_path = r\"F:\\projects\\coursera\\ibm data engineer_ETL_project\\ETL_Result\"\n",
    "expected_columns = ['name', 'height', 'weight'] # change columns manually each time depending on the files\n",
    "\n",
    "# Generate timestamp for output file\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M')\n",
    "\n",
    "# Name of output file with timestamp\n",
    "output_file_name = f\"appended_output_{timestamp}.csv\"\n",
    "output_file_path = os.path.join(target_path, output_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1de4717-ced2-465d-a18e-02cf9f2a91dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging settings\n",
    "\n",
    "log_path = os.path.join(target_path, \"etl_append_log.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a4071-0a30-4981-a314-a9064ecbcd8b",
   "metadata": {},
   "source": [
    "## Class Definition: ETLAppend\n",
    "\n",
    "This class handles the ETL process for structured files (CSV, JSON, and XML).  \n",
    "It reads each supported file format from the specified source directory, ensures consistency based on the expected columns, appends all data vertically into one unified DataFrame, and finally saves the result as a CSV file in the target directory.\n",
    "\n",
    "### Key Features:\n",
    "- Reads CSV, JSON (line-delimited), and XML files\n",
    "- Supports excluding files by name\n",
    "- Combines data based on consistent column names\n",
    "- Saves output with optional timestamp-based naming\n",
    "- Logs each step of the process for traceability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a11ea-3459-40f9-bb0d-e72b5b66e608",
   "metadata": {},
   "source": [
    "class ETLAppend:\n",
    "    def __init__(self, source_path, target_path, expected_columns):\n",
    "        self.source_path = source_path\n",
    "        self.target_path = target_path\n",
    "        self.columns = expected_columns\n",
    "        logging.info(\"ETLAppend initialized.\")\n",
    "\n",
    "    def read_csv(self, file_path):\n",
    "        logging.info(f\"Reading CSV file: {file_path}\")\n",
    "        return pd.read_csv(file_path)\n",
    "\n",
    "    def read_json(self, file_path):\n",
    "        logging.info(f\"Reading JSON file: {file_path}\")\n",
    "        return pd.read_json(file_path, lines=True)\n",
    "\n",
    "    def read_xml(self, file_path):\n",
    "        logging.info(f\"Reading XML file: {file_path}\")\n",
    "        df = pd.DataFrame(columns=self.columns)\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        for element in root:\n",
    "            row = {}\n",
    "            for col in self.columns:\n",
    "                el = element.find(col)\n",
    "                row[col] = float(el.text) if col in ['height', 'weight'] else el.text\n",
    "                row[col] = row[col] if row[col] is not None else \"\"\n",
    "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "        return df\n",
    "\n",
    "    def read_all_files(self, exclude_file=None):\n",
    "        logging.info(\"Started reading all files in source directory.\")\n",
    "        combined_df = pd.DataFrame(columns=self.columns)\n",
    "        readers = [(\"*.csv\", self.read_csv), (\"*.json\", self.read_json), (\"*.xml\", self.read_xml)]\n",
    "        for pattern, reader in readers:\n",
    "            for file_path in glob.glob(os.path.join(self.source_path, pattern)):\n",
    "                if exclude_file and os.path.basename(file_path) == exclude_file:\n",
    "                    logging.info(f\"Skipping file: {file_path}\")\n",
    "                    continue\n",
    "                df = reader(file_path)\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "        logging.info(\"Finished combining all files.\")\n",
    "        return combined_df\n",
    "\n",
    "    def save_to_csv(self, df, file_name=None):\n",
    "        os.makedirs(self.target_path, exist_ok=True)\n",
    "        if not file_name:\n",
    "            today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            file_name = f\"data_output_{today}.csv\"\n",
    "        output_path = os.path.join(self.target_path, file_name)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        logging.info(f\"Data saved to: {output_path}\")\n",
    "        print(\"Data saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5751e100-8d07-4aa1-a9af-1b8c303a895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLAppend() :\n",
    "    def __init__(self, source_path, target_path, columns) :\n",
    "        self.source_path = source_path\n",
    "        self.output_file_path = output_file_path\n",
    "        \n",
    "        self.columns = columns\n",
    "        logging.info(\"ETL Append Job Started.\")\n",
    "\n",
    "        # Automatically run the full ETL pipeline upon object creation\n",
    "        self.run()  \n",
    "\n",
    "\n",
    "    def extract_from_csv(self, file_path):\n",
    "        logging.info(f\"Reading CSV: {file_path}\")\n",
    "        return pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "    def extract_from_json(self, file_path):\n",
    "        logging.info(f\"Reading JSON: {file_path}\")\n",
    "        return pd.read_json(file_path, lines=True)\n",
    "\n",
    "\n",
    "    def extract_from_xml(self, file_path):\n",
    "        logging.info(f\"Reading XML: {file_path}\")\n",
    "        df = pd.DataFrame(columns=self.columns)\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        for person in root:\n",
    "            row = {}\n",
    "            for col in self.columns:\n",
    "                el = person.find(col)\n",
    "                row[col] = float(el.text) if col in ['height', 'weight'] else el.text\n",
    "            df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def extract_all(self) :\n",
    "        logging.info(\"Starting data extraction\")\n",
    "        combined_df = pd.DataFrame(columns = self.columns)\n",
    "        extractors = [\n",
    "                        (\"*.csv\", self.extract_from_csv),\n",
    "                        (\"*.json\", self.extract_from_json),\n",
    "                        (\"*.xml\", self.extract_from_xml)\n",
    "            \n",
    "                      ]\n",
    "        for pattern, extractor in extractors :\n",
    "            for file_path in glob.glob(os.path.join(self.source_path, pattern)) :\n",
    "                df = extractor(file_path)\n",
    "                if not df.empty :\n",
    "                    combined_df = pd.concat([combined_df, df] , ignore_index=True)\n",
    "        logging.info(\"Extraction phase ended.\")\n",
    "        return combined_df\n",
    "\n",
    "\n",
    "    def transform (self, df) :\n",
    "        logging.info(\"Transform phase started.\")\n",
    "        \n",
    "        df['height'] = round((df['height'] * 0.0254), 2)\n",
    "        df['weight'] = round((df['weight'] * 0.45359237), 2)\n",
    "        \n",
    "        logging.info(\"Transform phase ended.\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def load (self, appended_df) :\n",
    "        appended_df.to_csv(self.output_file_path, index=False)\n",
    "\n",
    "        logging.info(f\"Load phase completed. File saved to {target_path}\")\n",
    "        print(f\"Data saved to: {target_path}\")\n",
    "\n",
    "\n",
    "    def run (self) :\n",
    "        logging.info(\"ETL Job Started.\")\n",
    "        \n",
    "        df_extracted = self.extract_all()\n",
    "        df_transformed = self.transform(df_extracted)\n",
    "        self.load(df_transformed)\n",
    "\n",
    "        logging.info(\"ETL Job Ended.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd3addda-61f2-471c-88af-c0db9fbbd3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aliah\\AppData\\Local\\Temp\\ipykernel_15324\\3795023754.py:50: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat([combined_df, df] , ignore_index=True)\n",
      "C:\\Users\\aliah\\AppData\\Local\\Temp\\ipykernel_15324\\3795023754.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
      "C:\\Users\\aliah\\AppData\\Local\\Temp\\ipykernel_15324\\3795023754.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
      "C:\\Users\\aliah\\AppData\\Local\\Temp\\ipykernel_15324\\3795023754.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to: F:\\projects\\coursera\\ibm data engineer_ETL_project\\ETL_Result\n"
     ]
    }
   ],
   "source": [
    "etl = ETLAppend(source_path, target_path, expected_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf4a0dd-835c-4146-b4b4-80f336af93a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook etl_append_demo.ipynb to script\n",
      "[NbConvertApp] Writing 7176 bytes to etl_append_demo.py\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30e56c-7c33-4278-95c7-210e5c1cf18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
